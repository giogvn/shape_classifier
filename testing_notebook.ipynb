{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import json,os, cv2\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n","from util import Pipelines, VGG16Processor, ImageSorter\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:10:58.386413Z","iopub.status.busy":"2023-05-02T18:10:58.385695Z","iopub.status.idle":"2023-05-02T18:10:59.629153Z","shell.execute_reply":"2023-05-02T18:10:59.627724Z","shell.execute_reply.started":"2023-05-02T18:10:58.386372Z"},"trusted":true},"outputs":[],"source":["#Tests Configurations\n","\n","'''Important Paths to get and save results'''\n","TEST_DATA_PATH = Path(\"test_by_category\")\n","CLASSIFICATION_PERFORMANCE_PATH = Path(\"results/\")\n","CONFUSION_MATRICES_PATH = CLASSIFICATION_PERFORMANCE_PATH /'confusion_matrices'\n","RESULTS_FILE_NAME = CLASSIFICATION_PERFORMANCE_PATH / 'new_results.json'\n","\n","'''Informations about the \n","\n","VGG16 Classification Model'''\n","MODEL_TYPE = 'Fine Tuned'\n","MODEL_NAME = \"fine_tuned_text_and_no_text_are_diff_v2.hdf5\"\n","\n","RESULTS = {}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Classification Performance With different Datasets\n","\n","The following datasets all have images with and without text. They will be used to test the Not Fine Tuned and the Fine Tuned versions of the VGG16Models generated above to check how well they detect text in each of these categories:\n","\n","- *ct_kidney_imgs*: computed tomography images of human kidneys \n","- *digital_radiography_covid*: radiography images of human chests\n","- *magnetic_ressonance_brain*: magnetic ressonance images of human cancerigenous brains\n","- *mammography*: human mammography images\n","- *ultrasound_breast*: healthy and cancerigenous human breast ultrasound images\n","- *our_imgs*: real-world images extracted with the application"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sorted_imgs = []\n","vgg16_processor = VGG16Processor(MODEL_NAME, sorted_imgs)\n","\n","categories = os.listdir(TEST_DATA_PATH)\n","by_ctg_results = {}\n","\n","for ctg in categories:  \n","    ctg_path = TEST_DATA_PATH / ctg\n","    vgg16_processor.tests_path = ctg_path\n","\n","    ctg_results = vgg16_processor.classify_from_dir()\n","    true_classes = ctg_results['True Classes']\n","    pred_classes = ctg_results['Pred_classes']\n","\n","    vgg_acc = accuracy_score(true_classes, pred_classes)\n","    f1 = f1_score(true_classes, pred_classes)\n","    precision = precision_score(true_classes, pred_classes)\n","    recall = recall_score(true_classes, pred_classes)\n","\n","    cm = confusion_matrix(true_classes, pred_classes)\n","    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['No Text', 'With Text'])\n","    cm_display.plot()\n","    plt.savefig(CONFUSION_MATRICES_PATH / (str(ctg) + '_results.png'))\n","    \n","    ctg_results = {'Total Number of Images Tested': len(true_classes),\n","                   'Accuracy' : vgg_acc, 'Recall': recall, \n","                   'Precision' : precision, 'F1-Score': f1}\n","\n","    by_ctg_results[ctg] = ctg_results\n","    RESULTS['Results By Category'] = by_ctg_results\n","\n","with open(RESULTS_FILE_NAME, 'w') as f:\n","    json.dump(RESULTS, f, indent = 4)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Time Performance\n","\n","The following tests compare the execution times of the two pipelines performed with the images in the transformed_png_simples directory:\n","\n","- OCR-Only pipeline: every image in the directory is passed to pytesseract's image_to_string function\n","- Classification - OCR pipeline: every image is first classified as having or not text and only those positively classified are passed to pytesseract's image_to_string function\n","\n","The goal of such comparison is to tell if the overhead caused by the Classification task is significantly less than the one imposed by submitting images without text to OCR, since most images do not have text in it and therefore shouldn't be sent to the OCR pipeline."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#Tests Configurations\n","\n","'''Where the images to be submitted to the time tests are located'''\n","IMGS_DIR = Path('transformed_png_simple')\n","\n","'''The total number of images that should be submitted to the pipelines'''\n","TOTAL_IMGS = len(os.listdir(IMGS_DIR))\n","\n","'''The number of times the TOTAL_IMGS are going to be submitted to each\n","pipeline so their execution times can be averaged''' \n","N_TRIALS = 5 \n","\n","'''Test Results and Important Information about them'''\n","RESULTS = {}\n","           \n","RESULTS_FILE_NAME = CLASSIFICATION_PERFORMANCE_PATH / (str(TOTAL_IMGS) + '_images_results.json')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Images submitted to the pipelines: 2526\n","******************** Pipeline: VGG + OCR ********************\n","2526/2526 [==============================] - 841s 333ms/step\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: VGG + OCR: 947.880334299989\n","******************** Pipeline: OCR Only ********************\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: OCR Only: 1508.539592200017\n","******************** Pipeline: VGG + OCR ********************\n","2526/2526 [==============================] - 571s 226ms/step\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: VGG + OCR: 643.2382193999947\n","******************** Pipeline: OCR Only ********************\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: OCR Only: 1005.7863607999752\n","******************** Pipeline: VGG + OCR ********************\n","2526/2526 [==============================] - 375s 149ms/step\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: VGG + OCR: 430.85406419995707\n","******************** Pipeline: OCR Only ********************\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: OCR Only: 889.8955324999988\n","******************** Pipeline: VGG + OCR ********************\n","2526/2526 [==============================] - 409s 162ms/step\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: VGG + OCR: 459.2686254000291\n","******************** Pipeline: OCR Only ********************\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: OCR Only: 884.8380803000182\n","******************** Pipeline: VGG + OCR ********************\n","2526/2526 [==============================] - 428s 169ms/step\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: VGG + OCR: 500.81660889997147\n","******************** Pipeline: OCR Only ********************\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Giovani Andrade\\OneDrive - Poatek IT Consulting\\Documents\\siemens\\text_detection\\good_vgg16\\ocr_class\\ocr.py:334: RuntimeWarning: overflow encountered in ubyte_scalars\n","  median_value = (np.max(image) + np.min(image)) / 2\n"]},{"name":"stdout","output_type":"stream","text":["Time of Pipeline: OCR Only: 1378.1323963000323\n"]}],"source":["img_sorter = ImageSorter()\n","sorted_imgs = img_sorter.get_random_files_from_dir(IMGS_DIR, TOTAL_IMGS)\n","\n","sorted_imgs = [cv2.imread(str(img)) for img in sorted_imgs]\n","\n","print(f'Total Images submitted to the pipelines: {len(sorted_imgs)}')\n","\n","pipelines = Pipelines()\n","vgg_ocr_times = np.empty(N_TRIALS, dtype='float64')\n","ocr_only_times = np.empty(N_TRIALS, dtype='float64')\n","vgg16_processor = VGG16Processor(MODEL_NAME, sorted_imgs)\n","\n","for i in range(N_TRIALS):\n","    '''vgg16_processor.test_preprocess_images(sorted_imgs)'''\n","    print(\"*\"*20,f\"Pipeline: VGG + OCR\", \"*\"*20)\n","    vgg_ocr_times[i] = pipelines.classification_ocr_pipeline(MODEL_NAME, sorted_imgs)\n","    print(f'Time of Pipeline: VGG + OCR: {vgg_ocr_times[i]}')\n","    print(\"*\"*20,f\"Pipeline: OCR Only\", \"*\"*20)\n","    ocr_only_times[i] = pipelines.ocr_only_pipeline(sorted_imgs)\n","    print(f'Time of Pipeline: OCR Only: {ocr_only_times[i]}')\n","\n","classification_ocr_time = np.mean(vgg_ocr_times)\n","ocr_only_time =  np.mean(ocr_only_times)\n","time_diff  = (ocr_only_time - classification_ocr_time) / ocr_only_time\n","\n","RESULTS['total_number_of_imgs'] = TOTAL_IMGS\n","RESULTS['average_execution_times'] = {}\n","RESULTS['average_execution_times']['ocr_only_pipeline'] = ocr_only_time\n","RESULTS['average_execution_times']['classification_ocr_pipeline'] = classification_ocr_time\n","RESULTS['average_execution_times']['Enhancement'] = time_diff\n","\n","with open(RESULTS_FILE_NAME, 'w') as f:\n","    json.dump(RESULTS, f, indent=4) "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
